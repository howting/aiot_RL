#  AIOR_RL
## 研究動機
在現今的社會中，許多產業都變成智慧化由電腦來做主要操控，來避免「人」這個不確定因素，而在眾多人每天使用的工具"汽車"也不例外，正是人這個不確定因素所造成的傷害比比皆是，可以說汽車是必須智慧化的產業。汽車因科技的快速發展及人們對於攸關安全的主被動設備需求日以俱增，各大汽車公司都投身於製造自駕車，然而目前依據國際汽車工程 師學會（SAE International）所提出來的標準，市面上大部分的自駕車都是 Level2 的等級，主要都是以輔助系統，例如前碰預警（FCW）、主動式巡航定速（ACC）結合自動跟車和車道偏 離警示（LDW）搭配自動緊急煞停系統（AEB）和盲點偵測（BLIS），皆是以幫助行車安全與 減低駕駛疲勞為主，只有少數可達到 Level3 或 Level4 等級如 Google 的 Waymo、Uber 的自 駕車都具備 LV4 等級的能力，可設定目的地後自動將乘客載送到指定區域。除了這些絕大部 分還是需要駕駛者手動駕駛系統無法處理的部分(台灣機車太多)，但當可以準確依據未來行 車動向及情況並預測方向盤轉向角度及汽車加速度等等，就離 Level3 或 Level4 等級更進一步。
![CAR](https://www.gonews.com.tw/wp-content/uploads/2020/09/6%E5%A4%A7%E8%87%AA%E5%8B%95%E9%A7%95%E9%A7%9B_%E8%B3%87%E8%A8%8A%E4%B8%80%E8%A6%BD%E8%A1%A8-%E6%8B%B7%E8%B2%9D.jpg)  


 本小組的主要目的是探索人工智慧的實際應用。自動駕駛汽車領域的發展得到了廣泛的宣傳和認可。然而，這些進步並未真正滿足商業和社會需求。通過機器學習、計算機視覺、強化學習和神經網絡等應用人工智慧各個領域的進步，可以生產自動駕駛汽車，以改善人類社會。本文使用這些方法，將有助於討論該領域的發展。使用正強化來激勵車輛保持在所需的路徑上，這類似於為自動駕駛汽車開發的方法。同樣，本小組還有助於進一步探索不同學科和用途的機器學習發展。例如，這項研究的方法和結果可以應用於其他領域，如自然語言處理和計算機遊戲的強化學習 
 
 ## 方法
### Gym
由OpenAI推出，是一個Reinforcement Learning的套件包。該數據庫已被放在一起供開發人員使用各種人工智能技術，例如強化學習和計算機視覺來解決這些環境問題。許多用戶生成的這些任務解決方案都是公開可用的，從而為未來的發展提供了一個基準。本研究中探索的環境是 CarRacing-v0，這是一種 2D 自動駕駛汽車環境。
### CNN
CNN是一種前饋神經網路（feedforward neural network），它的人工神經元可以回應一部分覆蓋範圍內的周圍單元，對於大型圖像處理有出色表現。由一個或多個卷積層和頂端的全 連通層組成，同時也包括關聯權重和池化層（pooling layer）。這一結構使得卷積神經網路能夠利用輸入資料的二維結構。與其他深度學習結構相比，卷積神經網路在圖像和語音辨識方面能夠給出更好的結果。
### RL
RL是在機器學習(Machine learning, ML)的較為新穎的技術，與傳統預先處理資料集的方式相比，RL強調如何基於當前的環境來行動，進而取得最大的效益，其靈感來自於心理學中的行為主義理論，即代理者(Agent)該如何在環境給與獎勵或懲罰的刺激下，逐漸自我學習一套能夠對環境刺激的預期，且能獲得最大利益的行為準則，RL基礎架構圖一所示。
![RL](https://miro.medium.com/max/700/0*5dzxvNCfwmFRmvkp.jpg)  

#### Q-Learning
Q-learning: Q-學習是強化學習的一種方法。Q-學習利用表格來記錄所學習過的策略，透過不斷更新來使得智能體什麼情況下採取什麼行動會有最大的獎勵值。Q-學習不需要對環境進行建模，即使是對帶有隨機因素的轉移函數或者獎勵函數也不需要進行特別的改動就可以進行。

#### DQN
Deep Q-learning[5]:這是一個由DeepMind公司開發的利用深度卷積神經網絡來取代表格來進行Q-學習的算法。在使用非線性函數逼近的時候，強化學習經常會有不穩定性或者發散性，這種不穩定性來於當前的觀測中有比較強的自相關。DeepMind 通過使用經歷回放，也就是每次學習的時候並不直接從最近的經歷中學習，而是從之前的經歷中隨機採樣來進行訓練。
簡單來說DQN只是Q-Learning 的改進變體，使用深度捲積神經網絡作為逼近最佳動作的工具。選擇Q值最高的動作來最大化我們的獎勵。
Q(s,a) = r(s,a) + γ(maxQ(s',A))
	s 是當前狀態
	s' 是下一個未來狀態
	a 是特定的動作
	A 是動作空間
	Q(s,a)是a在狀態期間採取行動給出的 Q 值s
	r(s,a)是a在狀態期間採取行動所給予的獎勵s
	maxQ(s',A)是A在狀態期間在動作空間中採取任何動作給出的最大 Q 值s'
	γ 是對未來 Q 值進行貼現的貼現率，因為未來 Q 值不太重要
Deep Q Network（DQN）結構如圖2所示，以當前CarRacing的連續3個頂視圖作為當前狀態的輸入，並輸入每個Q值。輸入的shape為96x96x3其中的3並不是rgb，在此問題rgb並不重要，該3為連續3個頂視圖為灰階(維度為1維)，卷積層提取圖像特徵，最大池化層用於保持需要特徵，減少不必要的特徵，使用12個動作做為Q值。

