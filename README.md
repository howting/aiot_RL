#  AIOR_RL
## 研究動機
在現今的社會中，許多產業都變成智慧化由電腦來做主要操控，來避免「人」這個不確定因素，而在眾多人每天使用的工具"汽車"也不例外，正是人這個不確定因素所造成的傷害比比皆是，可以說汽車是必須智慧化的產業。汽車因科技的快速發展及人們對於攸關安全的主被動設備需求日以俱增，各大汽車公司都投身於製造自駕車，然而目前依據國際汽車工程 師學會（SAE International）所提出來的標準，市面上大部分的自駕車都是 Level2 的等級，主要都是以輔助系統，例如前碰預警（FCW）、主動式巡航定速（ACC）結合自動跟車和車道偏 離警示（LDW）搭配自動緊急煞停系統（AEB）和盲點偵測（BLIS），皆是以幫助行車安全與 減低駕駛疲勞為主，只有少數可達到 Level3 或 Level4 等級如 Google 的 Waymo、Uber 的自 駕車都具備 LV4 等級的能力，可設定目的地後自動將乘客載送到指定區域。除了這些絕大部 分還是需要駕駛者手動駕駛系統無法處理的部分(台灣機車太多)，但當可以準確依據未來行 車動向及情況並預測方向盤轉向角度及汽車加速度等等，就離 Level3 或 Level4 等級更進一步。
![CAR](https://www.gonews.com.tw/wp-content/uploads/2020/09/6%E5%A4%A7%E8%87%AA%E5%8B%95%E9%A7%95%E9%A7%9B_%E8%B3%87%E8%A8%8A%E4%B8%80%E8%A6%BD%E8%A1%A8-%E6%8B%B7%E8%B2%9D.jpg)  


 本小組的主要目的是探索人工智慧的實際應用。自動駕駛汽車領域的發展得到了廣泛的宣傳和認可。然而，這些進步並未真正滿足商業和社會需求。通過機器學習、計算機視覺、強化學習和神經網絡等應用人工智慧各個領域的進步，可以生產自動駕駛汽車，以改善人類社會。本文使用這些方法，將有助於討論該領域的發展。使用正強化來激勵車輛保持在所需的路徑上，這類似於為自動駕駛汽車開發的方法。同樣，本小組還有助於進一步探索不同學科和用途的機器學習發展。例如，這項研究的方法和結果可以應用於其他領域，如自然語言處理和計算機遊戲的強化學習 
 
 ## 方法
### Gym
由OpenAI推出，是一個Reinforcement Learning的套件包。該數據庫已被放在一起供開發人員使用各種人工智能技術，例如強化學習和計算機視覺來解決這些環境問題。許多用戶生成的這些任務解決方案都是公開可用的，從而為未來的發展提供了一個基準。本研究中探索的環境是 CarRacing-v0，這是一種 2D 自動駕駛汽車環境。
### CNN
CNN是一種前饋神經網路（feedforward neural network），它的人工神經元可以回應一部分覆蓋範圍內的周圍單元，對於大型圖像處理有出色表現。由一個或多個卷積層和頂端的全 連通層組成，同時也包括關聯權重和池化層（pooling layer）。這一結構使得卷積神經網路能夠利用輸入資料的二維結構。與其他深度學習結構相比，卷積神經網路在圖像和語音辨識方面能夠給出更好的結果。
### RL
RL是在機器學習(Machine learning, ML)的較為新穎的技術，與傳統預先處理資料集的方式相比，RL強調如何基於當前的環境來行動，進而取得最大的效益，其靈感來自於心理學中的行為主義理論，即代理者(Agent)該如何在環境給與獎勵或懲罰的刺激下，逐漸自我學習一套能夠對環境刺激的預期，且能獲得最大利益的行為準則，RL基礎架構圖一所示。
![RL](https://miro.medium.com/max/700/0*5dzxvNCfwmFRmvkp.jpg)  

#### Q-Learning
Q-learning: Q-學習是強化學習的一種方法。Q-學習利用表格來記錄所學習過的策略，透過不斷更新來使得智能體什麼情況下採取什麼行動會有最大的獎勵值。Q-學習不需要對環境進行建模，即使是對帶有隨機因素的轉移函數或者獎勵函數也不需要進行特別的改動就可以進行。

#### DQN
Deep Q-learning:這是一個由DeepMind公司開發的利用深度卷積神經網絡來取代表格來進行Q-學習的算法。在使用非線性函數逼近的時候，強化學習經常會有不穩定性或者發散性，這種不穩定性來於當前的觀測中有比較強的自相關。DeepMind 通過使用經歷回放，也就是每次學習的時候並不直接從最近的經歷中學習，而是從之前的經歷中隨機採樣來進行訓練。
簡單來說DQN只是Q-Learning 的改進變體，使用深度捲積神經網絡作為逼近最佳動作的工具。選擇Q值最高的動作來最大化我們的獎勵。
Q(s,a) = r(s,a) + γ(maxQ(s',A))
	s 是當前狀態
	s' 是下一個未來狀態
	a 是特定的動作
	A 是動作空間
	Q(s,a)是a在狀態期間採取行動給出的 Q 值s
	r(s,a)是a在狀態期間採取行動所給予的獎勵s
	maxQ(s',A)是A在狀態期間在動作空間中採取任何動作給出的最大 Q 值s'
	γ 是對未來 Q 值進行貼現的貼現率，因為未來 Q 值不太重要
Deep Q Network（DQN）結構如圖2所示，以當前CarRacing的連續3個頂視圖作為當前狀態的輸入，並輸入每個Q值。輸入的shape為96x96x3其中的3並不是rgb，在此問題rgb並不重要，該3為連續3個頂視圖為灰階(維度為1維)，卷積層提取圖像特徵，最大池化層用於保持需要特徵，減少不必要的特徵，使用12個動作做為Q值。

![DQN](https://github.com/howting/aiot_RL/blob/main/271392482_517707269499426_2485185398425252968_n.png?raw=true)

### donkey car
本計畫研究議題相當重要並具有影響力，2018 年 12 月立法院三讀通過《無人載具科技 創新實驗條例》，明訂結合人工智慧無人駕駛的車輛、船舶、航空器的移動載具可申請實驗，於核准範圍內可排除道路交通管理處罰條例、船舶法、民用航空法部分條文規範，鼓勵投入無人載具實驗。像是 Google 無人車、無人觀光船都可以上路測試。以無人車來說，無人車產業包括軟體和硬體是上兆美元的產業。因此更需要相關的人才，另外一方面，行政院在台南高鐵站特定區新建國內首座封閉式自駕車試驗場域。
目前這個領域最有名的自動駕駛模型車有麻省理工學院的 Duckiebot 小鴨車、Donkey Car 驢車、NVIDIA 的 JetBot、亞馬遜的 AWS DeepRacer、Self Driving RC Car等。 這些都是開放原始碼，為感興趣者提供充滿創意、樂趣的學習自動駕駛與人工智慧。
Donkey Car是Adam Conway在2017年發起的專案，他們一開始是想做一個縮小版的自走車，因此使用1/10比例的RC車作為車體，並用OpenCV裡的車道跟隨技術，但他的夥伴Will Roscoe希望能像Google和Tesla一樣可以使用深度學習技術打造自動駕駛系統，因此Donkey Car就加入了神經網路模型。
Donkey Car是一個開源機器學習的自走車專案，車上唯一的感測器就是相機。藉由操作Donkey Car，我們可以將機器學習的過程跑一遍，包括訓練資料蒐集與處理、模型選擇與調整、實機訓練、測試驗證等。從無到有瞭解機器學習的原理，並操作整個過程，最後透過車輛的自走看到實際的成果。而最有趣的地方不只是機器學習的部份，我們可以自由的改裝車上的硬體或是修改車道的設計比較各項變化，這都能讓我們離真實的自駕車更靠近。
#### VAE
![VAE](https://github.com/howting/aiot_RL/blob/main/VAE/1.png?raw=true)
#### PPO
![PPO](https://github.com/howting/aiot_RL/blob/main/PPO/2.png?raw=true)
#### 實驗結果
![VAE](https://github.com/howting/aiot_RL/blob/main/1.png?raw=true)
![PPO](https://github.com/howting/aiot_RL/blob/main/2.png?raw=true)
## 結論
在DQN的訓練中車子會隨著學習的迭帶次數增加，而逐漸的變化，因為要取得更多的獎勵，開始會抄捷徑，為了以更快的速度轉彎而有些許的偏離跑道。然而因為每一個回合中的地圖都是隨機生成的，容易導致DQN原本所學習的策略無法即時進行調整，因此訓練過程中的獎勵值浮動會非常的大，若是剛好遇到不是用的地圖可能就會得到非常低的獎勵，必須重新學習。
